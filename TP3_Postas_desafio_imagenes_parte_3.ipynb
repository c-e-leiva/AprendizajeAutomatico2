{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c-e-leiva/AprendizajeAutomatico2/blob/main/TP3_Postas_desafio_imagenes_parte_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **⚙️ Configuración del entorno: Activación de GPU**\n",
        "\n",
        "Para acelerar el entrenamiento del modelo, se habilitó el uso de GPU en el entorno de ejecución. Esto permite procesar grandes volúmenes de datos de forma mucho más eficiente, especialmente útil en tareas de aprendizaje profundo.\n",
        "\n",
        "#### ✅ Cómo activar la GPU en Google Colab:\n",
        "\n",
        "1. Ir al menú superior: `Entorno de ejecución` → `Cambiar tipo de entorno de ejecución`.\n",
        "2. En el apartado `Acelerador de hardware`, seleccionar `GPU`.\n",
        "3. Presionar `Guardar`.\n",
        "\n",
        "Una vez hecho esto, se puede verificar si la GPU está activa y detectar su nombre con el siguiente código en PyTorch:\n"
      ],
      "metadata": {
        "id": "HPz5cJDPH4aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"✅ GPU activa:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"❌ GPU no disponible\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBA6V2isFiic",
        "outputId": "1377d598-a09a-400e-9189-17ae9adc9039"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GPU activa: ❌ GPU no disponible\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh5mswsF7dJg"
      },
      "source": [
        "# **Clasificación de Imágenes con PyTorch y redes convolucionales**\n",
        "\n",
        "**TRABAJO EN EQUIPO POR POSTAS**\n",
        "\n",
        "#**POSTA 1: NORBERTO MARTEARENA URQUIZA - PASO 1**\n",
        "\n",
        "#**POSTA 2: MARCOS GALLO - PASO 2**\n",
        "\n",
        "#**POSTA 3: CARLOS EZEQUIEL LEIVA - PASO 3**\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "Este trabajo práctico tiene como objetivo evaluar su capacidad para:\n",
        "- Implementar un data loader personalizado en PyTorch\n",
        "- Diseñar e implementar redes neuronales convolucionales\n",
        "- Entrenar y evaluar modelos de aprendizaje profundo\n",
        "- Documentar adecuadamente el proceso y resultados de cada consigna.\n",
        "\n",
        "## Parte 1: Clasificador de Perros y Gatos con CNN\n",
        "\n",
        "### Datos\n",
        "He utilizado el dataset \"Cats and Dogs\" de Microsoft disponible en el sitio oficial, para ello utilice el generador de colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqZNRrYQseof"
      },
      "source": [
        "BREVE INTRODUCCIÓN a mi Posta (Posta 1)\n",
        "\n",
        "En este primer paso del trabajo práctico me enfoque en preparar y explorar el dataset que será utilizado para entrenar una red neuronal convolucional (CNN). El objetivo es dejar listo un conjunto de datos limpio, balanceado, transformado y estructurado en DataLoaders que puedan ser utilizados por el resto del equipo en las siguientes etapas.\n",
        "\n",
        "Para ello:\n",
        "- Se implementó un dataset personalizado basado en `torch.utils.data.Dataset`.\n",
        "- Se incorporaron transformaciones básicas (normalización, redimensionamiento, data augmentation).\n",
        "- Se parametrizaron variables para facilitar pruebas con distintos tamaños de imagen o subconjuntos de datos.\n",
        "- Se dividió el dataset en entrenamiento, validación y prueba (70/15/15).\n",
        "- Se realizó una exploración inicial visual y cuantitativa para verificar la correcta distribución de clases y el formato de los tensores.\n",
        "\n",
        "Este trabajo está pensado para facilitar el trabajo en equipo y permitir ajustes simples mediante la modificación de variables globales sin tocar el cuerpo principal del código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utu8TEs_-Vki"
      },
      "source": [
        "#**PASO 1 - POSTA 1. Preprocesamiento y Data Loader. [Nombre de quien resuelve: Norberto Martearena Urquiza]**\n",
        "\n",
        "-1.1 Descargue y descomprima el dataset.\n",
        "\n",
        "-1.2 Implemente su propio data loader personalizado utilizando las clases Dataset y DataLoader de PyTorch.\n",
        "\n",
        "-1.2.1 Incluya transformaciones apropiadas para las imágenes (redimensionamiento, normalización, data augmentation, etc.)\n",
        "\n",
        "-1.2.2 Divida los datos en conjuntos de entrenamiento, validación y prueba (sugerencia: 70%, 15%, 15%).\n",
        "\n",
        "-1.3 Realice una exploración inicial de los datos (distribución de clases, dimensiones de las imágenes, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6YJIwAwCALT"
      },
      "source": [
        "#PASO 1.1 DESCARGA DEL archivo del sitio oficial kagglecatsanddogs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTE7qcUfqbmD"
      },
      "source": [
        "En esta sección descargamos y descomprimimos el dataset original completo desde el sitio oficial de Microsoft. La descarga y extracción se realiza con los comandos `wget` y `unzip`, respectivamente, generados por la opcion de colab.\n",
        "\n",
        "❗ Importante:\n",
        "Ni `wget` ni `unzip` permiten limitar directamente la cantidad de archivos (IMAGENES) que se descargan o descomprimen. Siempre se descarga y extrae el paquete completo.\n",
        "\n",
        "✅ Para trabajar con un subconjunto de imágenes (por ejemplo, 1000 gatos y 1000 perros), esta limitación debe aplicarse luego en el código Python, dentro del Dataset personalizado, mediante el parámetro `max_per_class`.\n",
        "\n",
        "✅ Para esta corrida, se asume el valor None, o sea, el valor por omision se le da el total de registros que trae originalmente, aunque este se puede modificar. Esta parametrizacion, se realizo en el PASO 1.2.1\n",
        "\n",
        "\n",
        "Esto permite controlar la cantidad de imágenes utilizadas **sin modificar el archivo comprimido original**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj1Na-mj9_49",
        "outputId": "1fe840c4-fccb-4f90-9595-0e01578a4a4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-02 20:23:07--  https://download.microsoft.com/download/3/e/1/3e1c3f21-ecdb-4869-8368-6deba77b919f/kagglecatsanddogs_5340.zip\n",
            "Resolving download.microsoft.com (download.microsoft.com)... 23.3.94.53, 2600:1408:c400:168a::317f, 2600:1408:c400:1680::317f\n",
            "Connecting to download.microsoft.com (download.microsoft.com)|23.3.94.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 824887076 (787M) [application/octet-stream]\n",
            "Saving to: ‘kagglecatsanddogs_5340.zip.1’\n",
            "\n",
            "kagglecatsanddogs_5 100%[===================>] 786.67M  68.8MB/s    in 12s     \n",
            "\n",
            "2025-06-02 20:23:20 (63.4 MB/s) - ‘kagglecatsanddogs_5340.zip.1’ saved [824887076/824887076]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# prompt: descargar https://download.microsoft.com/download/3/e/1/3e1c3f21-ecdb-4869-8368-6deba77b919f/kagglecatsanddogs_5340.zip\n",
        "\n",
        "!wget https://download.microsoft.com/download/3/e/1/3e1c3f21-ecdb-4869-8368-6deba77b919f/kagglecatsanddogs_5340.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zdGVeb_kOY"
      },
      "source": [
        "##PASO 1.1.1 DESCOMPRIMIR EL ARCHIVO\n",
        "\n",
        "REALIZO LA DESCOMPRESION DEL ARCHIVO BAJADO UTILIZANDO LAS NUEVAS FACILIDADES DE COLAB PARA LA GENERACION DE CODIGO  IGUAL QUE EN LA DESCARGA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMcnFL49_yvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "630f69f3-1e6d-45aa-96c1-75992abc7478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace PetImages/Cat/0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "\n",
        "# prompt: DESCOMPRIMO EL ARCHIVO ZIP BAJADO\n",
        "\n",
        "!unzip -q kagglecatsanddogs_5340.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5smbnQPsSsZC"
      },
      "source": [
        "q significa quiet (modo silencioso): evita mostrar en pantalla todos los archivos que está extrayendo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAOLHXc5TCa3"
      },
      "source": [
        "El archivo kagglecatsanddogs_5340.zip contiene una carpeta llamada PetImages/, con dos subcarpetas:\n",
        "\n",
        "PetImages/Cat/\n",
        "\n",
        "PetImages/Dog/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KymQE_PAU0DN"
      },
      "source": [
        "con el siguiente comando se muestran los archivos y carpetas en el directorio actual con detalles como tamaño, permisos, fecha y nombre.\n",
        "en este caso especifico -1, permite verificar visualmente que el archivo fue descomprimido correctamente y que la carpeta PetImages/ está disponible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG7EU9EjArEH"
      },
      "outputs": [],
      "source": [
        "! ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN2J3GxvBFay"
      },
      "outputs": [],
      "source": [
        "! ls -l PetImages/Dog/*.jpg | wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12zXhjcvVnt6"
      },
      "source": [
        "wc -1, es para confirmar que hay efectivamente 12.500 imágenes de perros (o las que quedaron tras la descompresión), y se puede verificar que tiene la misma cantidad la carpeta de gatos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXyqyiLsEWgs"
      },
      "source": [
        "*En esta tarea voy a cargar solo los nombres de los archivos .jpg, de la carpeta /Dog y de la carpeta /Cat, para que no me tome otros archivos que existan que no sean .jpg, como por ejemplo .DB*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZNSxzz4eKdF"
      },
      "outputs": [],
      "source": [
        "# prompt: cargar los nombres de archivos del directorio PetImages que sean jpg\n",
        "# aqui  Versión optimizada para listar archivos válidos por clase\n",
        "import os\n",
        "\n",
        "cat_files = [f for f in os.listdir('PetImages/Cat') if f.endswith('.jpg')]\n",
        "dog_files = [f for f in os.listdir('PetImages/Dog') if f.endswith('.jpg')]\n",
        "\n",
        "print(f\"Total imágenes .jpg en Cat: {len(cat_files)}\")\n",
        "print(f\"Total imágenes .jpg en Dog: {len(dog_files)}\")\n",
        "\n",
        "# Visualización de los primeros 10 archivos de cada clase\n",
        "print(\"Ejemplos Cat:\", cat_files[:10])\n",
        "print(\"Ejemplos Dog:\", dog_files[:10])\n",
        "\n",
        "\n",
        "# Este paso permite verificar si hay nombres inusuales, errores de codificación o archivos duplicados y se indica la cantidad de imagenes en cada sub carpeta\n",
        "# También ayuda a observar patrones como si hay imágenes con nombres como `0.jpg`, etc. Esto será útil al momento de cargarlos como datos etiquetados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "m2EHrU8cK_fv"
      },
      "outputs": [],
      "source": [
        "dog_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXeKWWlBOcfo"
      },
      "source": [
        "#**PASO 1.2**\n",
        "\n",
        "Implemente su propio data loader personalizado utilizando las clases Dataset y DataLoader de PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNlcKtm-QUsn"
      },
      "source": [
        "#**PASO 1.2 - Dataset personalizado**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRIYB96GpVeH"
      },
      "outputs": [],
      "source": [
        "# PASO 1.2 - Dataset personalizado\n",
        "# prompt: Crear una clase Dataset que cargue imágenes desde carpetas Cat y Dog con etiquetas automáticas y control de errores.\n",
        "# ----------------------------- Importar librerias\n",
        "import os\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O5AdsUqfkUn"
      },
      "source": [
        "En esta sección definimos una clase Dataset personalizada llamada GatosPerrosDataset que permite:\n",
        "\n",
        "cargar imágenes desde las carpetas PetImages/Cat y PetImages/Dog\n",
        "\n",
        "Asignar automáticamente etiquetas 0 a los gatos y 1 a los perros según el nombre de la carpeta\n",
        "\n",
        "Aplicar transformaciones a las imágenes (rescale, normalización, etc.)\n",
        "\n",
        "Manejar errores por imágenes corruptas usando try/except\n",
        "\n",
        "Esta clase es una subclase de torch.utils.data.Dataset y es fundamental para usar DataLoader luego.\n",
        "\n",
        "   \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRg8-N5sprgv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Creamos una clase heredada de torch.utils.data.Dataset\n",
        "# que permite cargar imágenes desde carpetas \"PetImages/Cat\" y \"PetImages/Dog\".\n",
        "# Se asignan etiquetas automáticas (0=gato, 1=perro) según el nombre de la carpeta.\n",
        "#\n",
        "# ⚠️ Se agregó un bloque try/except en __getitem__ para evitar errores por imágenes corruptas.\n",
        "# Si una imagen no se puede abrir, se elige aleatoriamente otra y se informa por consola.\n",
        "\n",
        "class GatosPerrosDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, max_per_class=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        clases = os.listdir(root_dir)\n",
        "\n",
        "        for idx, clase in enumerate(clases):\n",
        "            clase_path = os.path.join(root_dir, clase)\n",
        "            if not os.path.isdir(clase_path):\n",
        "                continue\n",
        "\n",
        "            archivos = [f for f in os.listdir(clase_path) if f.endswith('.jpg')]\n",
        "            if max_per_class:\n",
        "                archivos = archivos[:max_per_class]\n",
        "\n",
        "            for archivo in archivos:\n",
        "                self.data.append((os.path.join(clase_path, archivo), idx))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "          # Intentamos abrir la imagen. Si falla, probamos con otra aleatoria.\n",
        "          while True:\n",
        "              img_path, label = self.data[idx]\n",
        "              try:\n",
        "                  image = Image.open(img_path).convert(\"RGB\")\n",
        "                  if self.transform:\n",
        "                      image = self.transform(image)\n",
        "                  return image, label\n",
        "              except (UnidentifiedImageError, OSError) as e:\n",
        "                  print(f\"⚠️ Imagen inválida: {img_path} — {e}\")\n",
        "                  idx = torch.randint(0, len(self.data), (1,)).item()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA9iyo64TVO0"
      },
      "source": [
        "#**PASO 1.2.1  Incluya transformaciones apropiadas para las imágenes (redimensionamiento, normalización, data augmentation, etc.)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co3irnzkrEW-"
      },
      "outputs": [],
      "source": [
        "# prompt: Definir parámetros por defecto y transformaciones para preprocesar imágenes de entrada al modelo.\n",
        "\n",
        "# -----------------------------\n",
        "# PASO 1.2.1 - Transformaciones y configuraciones por defecto\n",
        "# -----------------------------\n",
        "\n",
        "# Defino variables configurables para permitir ajustes sin cambiar el código base\n",
        "image_size = 128              # Tamaño de imagen a usar (por defecto: 128x128)\n",
        "batch_size = 32               # Tamaño del lote de entrenamiento (modificable para pruebas de memoria o estabilidad)\n",
        "max_images_per_class = None   # Cantidad máxima de imágenes por clase (None usa todas, por ejemplo las 12500 por clase)\n",
        "\n",
        "# Aplicamos transformaciones comunes para entrenamiento:\n",
        "# - Resize: redimensiona todas las imágenes al tamaño definido (image_size x image_size)\n",
        "# - RandomHorizontalFlip: voltea horizontalmente algunas imágenes para aumentar variedad (data augmentation)\n",
        "# - ToTensor: convierte la imagen a tensor y normaliza a valores [0, 1]\n",
        "# - Normalize: normaliza los canales de color a [-1, 1] centrando en 0 para estabilizar el entrenamiento\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2njHlj0YF77"
      },
      "outputs": [],
      "source": [
        "# prompt: Cargar el dataset completo aplicando la clase personalizada con transformaciones y configuración definida.\n",
        "\n",
        "# -----------------------------\n",
        "# Cargar dataset completo\n",
        "# -----------------------------\n",
        "\n",
        "# El archivo ya se descargo y se descomprimió el ZIP en PetImages/ en el paso 1.1 y paso  1.1.1\n",
        "# Esta ruta puede adaptarse si el directorio cambia\n",
        "ruta_dataset = \"PetImages\"\n",
        "dataset = GatosPerrosDataset(root_dir=ruta_dataset, transform=data_transforms, max_per_class=max_images_per_class)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MjETT_GdbgX"
      },
      "source": [
        "#**PASO 1.2.2 - Dividir en entrenamiento, validación y prueba (70/15/15)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vug1_R8jrVCt"
      },
      "outputs": [],
      "source": [
        "# prompt: Dividir el dataset en subconjuntos de entrenamiento, validación y prueba utilizando random_split.\n",
        "# -----------------------------\n",
        "# PASO 1.2.2 - Dividir en entrenamiento, validación y prueba (70/15/15)\n",
        "# -----------------------------\n",
        "\n",
        "# Usamos torch.utils.data.random_split para dividir el conjunto\n",
        "longitud = len(dataset)\n",
        "entrenamiento, validacion, prueba = random_split(dataset, [int(0.7*longitud), int(0.15*longitud), longitud - int(0.7*longitud) - int(0.15*longitud)])\n",
        "\n",
        "# Creamos los dataloaders que se usarán en el entrenamiento del modelo\n",
        "train_loader = DataLoader(entrenamiento, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validacion, batch_size=batch_size)\n",
        "test_loader = DataLoader(prueba, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--EanK-keZf5"
      },
      "source": [
        "#**PASO 1.3 - Exploración de los datos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W-HN6_3pMW3"
      },
      "outputs": [],
      "source": [
        "# prompt: Explorar visualmente y cuantitativamente la distribución de clases y forma de los tensores en los dataloaders.\n",
        "# -----------------------------\n",
        "# PASO 1.3 - Exploración de los datos\n",
        "# -----------------------------\n",
        "\n",
        "# Analizamos la distribución de etiquetas (clases) en cada subconjunto\n",
        "labels_entrenamiento = [label for _, label in entrenamiento]\n",
        "labels_validacion = [label for _, label in validacion]\n",
        "labels_prueba = [label for _, label in prueba]\n",
        "\n",
        "print(\"Distribución de clases:\")\n",
        "print(\"Entrenamiento:\", Counter(labels_entrenamiento))\n",
        "print(\"Validación:\", Counter(labels_validacion))\n",
        "print(\"Prueba:\", Counter(labels_prueba))\n",
        "\n",
        "# Visualizamos las dimensiones de los tensores de imagen\n",
        "sample_imgs, _ = next(iter(train_loader))\n",
        "print(\"Forma del lote de entrenamiento:\", sample_imgs.shape)  # (batch_size, canales, alto, ancho)\n",
        "\n",
        "# Visualizamos un grupo de 10 imágenes del conjunto de entrenamiento\n",
        "fig, axs = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    img = sample_imgs[i].permute(1, 2, 0) * 0.5 + 0.5  # desnormalizamos para visualizar\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "plt.suptitle(\"Ejemplos de imágenes de entrenamiento\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFaDXDIViulH"
      },
      "source": [
        "Recomendaciones clave:\n",
        "\n",
        "Correr el notebook completo desde el principio para que la clase GatosPerrosDataset esté definida.\n",
        "No cambiar el nombre ni la estructura de carpetas: PetImages/Cat y PetImages/Dog deben estar presentes.\n",
        "Puede usar los siguientes valores por defecto:\n",
        "image_size = 128\n",
        "batch_size = 32\n",
        "max_per_class = 1000\n",
        "Las transformaciones sugeridas son:\n",
        "transformaciones = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "])\n",
        "🛡️ Consideraciones especiales:\n",
        "\n",
        "Las imágenes inválidas están controladas mediante try/except, pero si aparece el mensaje ⚠️ Imagen inválida, no significa que el código falló.\n",
        "Si se desea trabajar con menos datos, puede ajustarse max_per_class en el momento de instanciar el dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2doMruJIlre6"
      },
      "source": [
        "#**CONCLUSIONES**\n",
        "\n",
        "\n",
        "La implementación desarrollada en esta primera posta garantiza una base sólida para el entrenamiento de modelos de clasificación binaria con redes neuronales.\n",
        "\n",
        "De acuerdo a lo consensuado con la profesora la estructura modular y parametrizada permite que cualquier integrante del equipo pueda trabajar con distintas cantidades de datos, aplicar nuevas transformaciones o ajustar los hiperparámetros sin alterar el funcionamiento del conjunto.\n",
        "\n",
        "En futuras etapas, se podrá observar cómo el preprocesamiento influye directamente en el desempeño de los modelos CNN y MLP, como se vio en experiencias previas con subconjuntos reducidos de 500 imágenes por clase, donde se identificaron problemas de carga de clases. Esta versión mejora ese comportamiento mediante mayor control, limpieza y balanceo de datos.\n",
        "\n",
        "La configuración flexible y el control visual implementado también brindan herramientas valiosas para la detección de errores tempranos y el diagnóstico de la calidad del dataset.\n",
        "\n",
        "Como podran ver durante la ejecucion se detectaron dos imagenes que daban error y suspendian el proceso, por ese motivo se incorporaron las mejoras que permitieron omitir esas imagenes para una corrida normal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1EqNh1yX36S8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyVuaHu7TMi8"
      },
      "source": [
        "# **PASO 2 Arquitectura de la Red. [Resuelve: MARCOS GALLO]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWQZ9jE0b3o8"
      },
      "source": [
        "# **Consigna de la posta 2:**\n",
        "\n",
        "- Diseñe una red neuronal convolucional para la clasificación binaria (perros vs gatos)\n",
        "- La arquitectura debe utilizar exclusivamente capas convolucionales y fully-connected\n",
        "- Implemente correctamente el forward pass de la red\n",
        "- Justifique las decisiones de diseño (número de capas, filtros, activaciones, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBeqK_BxPCF-"
      },
      "source": [
        "## 🧠 Arquitectura de la Red Neuronal Convolucional (CNN) para clasificación de Perros vs Gatos\n",
        "\n",
        "Se creará una clase que implementa una arquitectura CNN, diseñada  para la tarea de clasificación binaria de imágenes de gatos y perros.\n",
        "\n",
        "Se buscó un diseño para facilitar el trabajo en equipo, flexible que permite modificar parámetros para experimentar con variantes.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 Arquitectura General\n",
        "\n",
        "La clase GatosPerrosCNN hereda de nn.Module y organiza su estructura compuesta por tres bloques convolucionales, seguidos por dos capas totalmente conectadas. Cada bloque realiza:\n",
        "\n",
        "- Convolución 2D (`Conv2D`)\n",
        "- Activación no lineal (por defecto: `ReLU`)\n",
        "- Reducción espacial mediante `MaxPooling`\n",
        "\n",
        "La parte final densa incluye:\n",
        "\n",
        "- Aplanado del tensor de activaciones\n",
        "- Capa fully connected intermedia con activación\n",
        "- Capa de salida con 1 sola neurona y activación Sigmoid para clasificación binaria (probabilidad entre 0 y 1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGNZow6HPxop"
      },
      "source": [
        "## 🔍 Parámetros configurables del modelo `GatosPerrosCNN`\n",
        "\n",
        "A continuación, se detallan los parámetros del constructor del modelo `GatosPerrosCNN`, con el objetivo de facilitar su comprensión, experimentación y adaptación por parte de todos los integrantes del equipo.  \n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Argumentos del constructor\n",
        "\n",
        "- **`num_filters: list[int]`**  \n",
        "  Lista que indica la cantidad de filtros en cada capa convolucional.  \n",
        "  Por ejemplo: `[16, 32, 64]` define 3 capas con 16, 32 y 64 filtros respectivamente.  \n",
        "  Esto permite una progresiva extracción de patrones: de bordes simples a formas más complejas.\n",
        "\n",
        "---\n",
        "\n",
        "- **`fc_hidden_units: int`**  \n",
        "  Número de neuronas en la capa `fully connected` intermedia.  \n",
        "  Une la parte convolucional (extracción de características) con la salida binaria (gato o perro).  \n",
        "  Comúnmente se usan valores como 64, 128 o 256.\n",
        "\n",
        "---\n",
        "\n",
        "- **`input_shape: tuple[int]`**  \n",
        "  Tamaño esperado de las imágenes de entrada en el formato `(canales, alto, ancho)`, por ejemplo `(3, 128, 128)` para imágenes RGB.  \n",
        "  Es fundamental para calcular automáticamente el tamaño de entrada a la capa `fully connected`.  \n",
        "  Aunque las imágenes del dataset se redimensionen a 128x128 por defecto en el preprocesamiento hecho en la Posta 1 (tambien es flexible), incluir este parámetro hace el código más robusto y adaptable.\n",
        "\n",
        "---\n",
        "\n",
        "- **`use_dropout: bool`**  \n",
        "  Si se establece en `True`, agrega una capa `Dropout` antes de la salida.  \n",
        "  Esto ayuda a **prevenir el sobreajuste** al desactivar aleatoriamente neuronas durante el entrenamiento.  \n",
        "  Es útil especialmente si se detecta un rendimiento pobre en validación.\n",
        "\n",
        "---\n",
        "\n",
        "- **`activation: Callable`**  \n",
        "  Función de activación utilizada después de cada capa convolucional y `fully connected`.  \n",
        "  Por defecto se usa `ReLU` (`torch.nn.ReLU()`), por su eficiencia y buen rendimiento promedio.  \n",
        "  También se podrían probar otras como:\n",
        "  - `LeakyReLU()`: evita que neuronas se “apaguen” totalmente.\n",
        "  - `tanh` o `sigmoid`: menos comunes, pero útiles en arquitecturas pequeñas o experimentales.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROMPT:\n",
        "# \"Por favor, crea una clase en Python que defina una red neuronal convolucional para clasificación binaria de imágenes (gatos vs perros).\n",
        "#La red debe tener 3 capas convolucionales con filtros de tamaño 16, 32 y 64.Cada capa convolucional debe estar seguida de una capa de max pooling.\n",
        "#Incluye una capa fully connected final con una salida de un valor entre 0 y 1 usando sigmoid. Incluye opciones para usar dropout y una función de\n",
        "#activación configurable, por ejemplo ReLU.La clase debe calcular automáticamente el tamaño de la entrada a las capas fully connected.\n",
        "#Define un método forward para pasar los datos a través de la red.\"\n",
        "\n",
        "\n",
        "# Importa librerías necesarias\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GatosPerrosCNN(nn.Module):\n",
        "    def __init__(self, num_filters=[16, 32, 64], fc_hidden_units=128, input_shape=(3, 128, 128),\n",
        "                 use_dropout=False, activation=F.relu):\n",
        "        super(GatosPerrosCNN, self).__init__()\n",
        "\n",
        "        self.activation = activation\n",
        "        self.use_dropout = use_dropout\n",
        "\n",
        "        # Capas convolucionales\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=num_filters[0], kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(num_filters[0], num_filters[1], kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(num_filters[1], num_filters[2], kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Cálculo del tamaño de entrada a FC\n",
        "        self._calculate_flatten_dim(input_shape)\n",
        "\n",
        "        # Capas fully-connected\n",
        "        self.fc1 = nn.Linear(self.flatten_dim, fc_hidden_units)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(fc_hidden_units, 1)\n",
        "\n",
        "    def _calculate_flatten_dim(self, input_shape):\n",
        "        c, h, w = input_shape\n",
        "        dummy_input = torch.zeros(1, c, h, w)\n",
        "        x = self.pool3(self.activation(self.conv3(\n",
        "            self.pool2(self.activation(self.conv2(\n",
        "                self.pool1(self.activation(self.conv1(dummy_input)))\n",
        "            )))\n",
        "        )))\n",
        "        self.flatten_dim = x.view(1, -1).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.activation(self.conv1(x)))\n",
        "        x = self.pool2(self.activation(self.conv2(x)))\n",
        "        x = self.pool3(self.activation(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.activation(self.fc1(x))\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "y62gUndxBTXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔍 Cálculo automático del tamaño de entrada a la capa fully connected\n",
        "\n",
        "\n",
        "\n",
        "De esta manera lo que se evita es tener errores cuando se cambia el tamaño de entrada o se modifican las capas convolucionales. La característica principal es que **el modelo calcula automáticamente el tamaño del vector que ingresa a la capa fully connected intermedia (fc1)**. Para ello, se pasa un tensor \"dummy\" de ceros a través de todas las capas convolucionales y de pooling, sin necesidad de calcular manualmente la reducción de tamaño producida por las capas de pooling.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "\n",
        "`dummy_input = torch.zeros(1, c, h, w)`  \n",
        "`x = ...` # pasar dummy_input por las capas convolucionales y pooling  \n",
        "`self.flatten_dim = x.view(1, -1).shape[1]`  "
      ],
      "metadata": {
        "id": "IcdUoLDmf8pH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### ⏩ Explicación del método `forward`\n",
        "\n",
        "El método `forward` define cómo los datos de entrada (`x`) pasan a través de la red neuronal en cada paso. Primero, la entrada se procesa en una secuencia de capas convolucionales: `conv1`, `conv2` y `conv3`. Después de cada convolución, se aplica una función de activación (`self.activation`) para introducir no linealidad, seguida de una capa de pooling (`pool1`, `pool2`, `pool3`) que reduce las dimensiones espaciales de las imágenes, ayudando a extraer características de diferentes niveles de abstracción.\n",
        "\n",
        "Luego, la salida de la última capa de pooling se aplana con `x.view(x.size(0), -1)`, convirtiendo el tensor en un vector para que pueda ingresarse a las capas totalmente conectadas (`fc1` y `fc2`).\n",
        "\n",
        "Antes de la última capa, se aplica nuevamente la función de activación, y, si `use_dropout` está habilitado, se activa.\n",
        "\n",
        "El parámetro `use_dropout` controla si se debe aplicar **regularización** por medio de `Dropout`, una técnica que reduce el sobreajuste al \"apagar\" aleatoriamente algunas neuronas durante el entrenamiento.\n",
        "\n",
        "Si `use_dropout=True`, el modelo aplicará una capa `Dropout(0.5)` justo después de la activación de la capa fully connected intermedia. Esta opción es útil cuando el modelo memoriza el entrenamiento pero falla en generalizar bien en validación.\n",
        "\n",
        "Finalmente, la salida pasa por la capa `fc2` y se aplica la función `sigmoid` para producir una probabilidad de clasificación binaria (perro vs gato).\n",
        "\n"
      ],
      "metadata": {
        "id": "i94UhPBSiq6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Uso recomendado:***\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Esta clase permite ser fácilmente modificada para comparar variantes de arquitectura:\n",
        "  - Cambiar número de capas o filtros\n",
        "  - Probar otras funciones de activación\n",
        "  - Añadir Dropout\n",
        "  - Ajustar el tamaño de imágenes de entrada\n",
        "\n"
      ],
      "metadata": {
        "id": "-8G6hpJQfM7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔍 Verificación inicial de la red: instanciación y prueba con input ficticio.\n",
        "\n",
        "Antes de entrenar una red neuronal convolucional, es fundamental verificar que la arquitectura esté correctamente implementada. Es lo que sigue en los siguientes pasos:"
      ],
      "metadata": {
        "id": "Nv8sdHpTBXTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ 1. Instanciación del modelo"
      ],
      "metadata": {
        "id": "J2Q4GYEpBkIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GatosPerrosCNN(input_shape=(3, 128, 128))\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "7s-JMf2VBjnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Quí se crea una instancia de la clase GatosPerrosCNN, especificando que las imágenes de entrada tienen 3 canales (RGB) y tamaño 128×128 píxeles.\n",
        "\n",
        "Este paso permite asegurarse de que los tamaños de entrada y salida entre capas están bien conectados. El `print(model)` muestra toda la arquitectura del modelo con sus capas, canales y dimensiones, lo que facilita la revisión en equipo o individual antes del entrenamiento."
      ],
      "metadata": {
        "id": "vLhdLwA1Braw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧪 2. Prueba con input \"dummy\" (aleatorio)"
      ],
      "metadata": {
        "id": "MLePBCX6BvnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 3, 128, 128)\n",
        "output_dummy = model(dummy_input)\n",
        "print(output_dummy)\n"
      ],
      "metadata": {
        "id": "hhVP0ubn6oln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero se crea un tensor ficticio con forma (1, 3, 128, 128) (una imagen RGB de prueba). Luego se lo pasa por el modelo y se imprime la salida.\n",
        "\n",
        "Este paso verifica el forward pass completo: se comprueba que la red acepta correctamente la entrada y devuelve una salida coherente. Dado que se trata de un problema de clasificación binaria, la salida debe ser un tensor de forma (1, 1) con valores entre 0 y 1 gracias a la activación sigmoid en la última capa.\n",
        "\n",
        "✅ Esta prueba detecta errores comunes como dimensiones incompatibles o errores de tipo, y permite trabajar con seguridad en la siguiente etapa de entrenamiento."
      ],
      "metadata": {
        "id": "Zy1EEMduB5AX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Visualización de una imagen dummy y su predicción"
      ],
      "metadata": {
        "id": "9bFC2e5yB81g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt: \"Dame un ejemplo de código para visualizar una imagen dummy y su predicción\"\n",
        "\n",
        "# Librerias necesarias\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Convertir imagen dummy a formato visualizable\n",
        "imagen = dummy_input[0].permute(1, 2, 0).detach().numpy()\n",
        "plt.imshow((imagen - imagen.min()) / (imagen.max() - imagen.min()))\n",
        "plt.title(f\"Predicción del modelo: {output_dummy.item():.4f}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q4cCOoC5CApA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qué hace este paso?\n",
        "\n",
        "`dummy_input[0]`: extrae la primera imagen del batch.\n",
        "\n",
        "`.permute(1, 2, 0)`: cambia los ejes para que se vea correctamente al graficar (altura, ancho, canales).\n",
        "\n",
        "`.detach().numpy()`: lo transforma en un array de NumPy para poder visualizarlo.\n",
        "\n",
        "`(imagen - imagen.min()) / (imagen.max() - imagen.min())`: normaliza los valores al rango [0, 1].\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "📌 ¿Qué muestra el gráfico?\n",
        "\n",
        "Una imagen completamente aleatoria (no es una muestra real del dataset). En el título del gráfico se muestra la salida del modelo.Ejemplo: Predicción del modelo: \"valor entre 0 y 1\", lo cual puede interpretarse como la probabilidad de que la imagen sea de un perro (1) o un gato (0).\n",
        "\n",
        "🧩 Esta verificación cumple un rol clave como validación estructural del modelo antes del entrenamiento. Aunque la predicción no tiene valor real aún (porque el modelo no ha sido entrenado), asegura que:\n",
        "\n",
        "- La arquitectura es funcional.\n",
        "\n",
        "- El forward pass se ejecuta sin errores.\n",
        "\n",
        "- Las dimensiones de entrada y salida son correctas.\n",
        "\n",
        "- Se puede visualizar la salida de forma interpretable.\n",
        "\n",
        "Este tipo de pruebas se toma como una buena práctica antes de configurar los hiperparámetros y ejecutar el entrenamiento completo.\n",
        "\n"
      ],
      "metadata": {
        "id": "HfdlrqjeCL9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Referencias utilizadas:***\n",
        "\n",
        "---\n",
        "- Prompt chatgpt\n",
        "\n",
        "a)   Por favor, crea una clase en Python que defina una red neuronal convolucional para clasificación binaria de imágenes (gatos vs perros).\n",
        "La red debe tener 3 capas convolucionales con filtros de tamaño 16, 32 y 64.Cada capa convolucional debe estar seguida de una capa de max pooling.\n",
        "Incluye una capa fully connected final con una salida de un valor entre 0 y 1 usando sigmoid. Incluye opciones para usar dropout y una función de\n",
        "activación configurable, por ejemplo ReLU.La clase debe calcular automáticamente el tamaño de la entrada a las capas fully connected.\n",
        "Define un método forward para pasar los datos a través de la red.\"\n",
        "\n",
        "b)   Dime puntos a tener en cuenta para lograr una estructura de codigo modular y flexible para un proyecto en equipo con consignas por \"postas\".\n",
        "\n",
        "c)   Dame un ejemplo de código para visualizar una imagen dummy y su predicción.\n",
        "\n",
        "- Bibliografía de la materia\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KWGhLr84QhX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__________________________\n",
        "\n",
        "___________________________"
      ],
      "metadata": {
        "id": "pSqI73v8vA37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PASO 3 Entrenamiento. [Resuelve: CARLOS E. LEIVA]**\n",
        "\n",
        "- 3.1 Configure los hiperparámetros del entrenamiento (learning rate, batch size, número de épocas, etc.)\n",
        "\n",
        "- 3.2 Seleccione una función de pérdida y un optimizador adecuados\n",
        "\n",
        "- .3.3 Implemente el bucle de entrenamiento completo\n",
        "\n",
        "- 3.4 Registre las métricas de entrenamiento y validación por época (precisión, recall, F1-score, etc.)\n",
        "\n",
        "- 3.5 Implemente early stopping basado en el rendimiento de validación"
      ],
      "metadata": {
        "id": "38PZanTwvGT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1. Configurar Hiperparámetros**\n",
        "En esta sección definimos los hiperparámetros fundamentales para el entrenamiento de la red neuronal. Estos parámetros controlan aspectos clave del proceso de optimización y afectan directamente el desempeño del modelo:\n",
        "\n",
        "- **Tasa de aprendizaje (learning_rate):** Indica qué tan grande es el paso que da el optimizador en cada actualización de pesos. Un valor demasiado alto puede hacer que el entrenamiento no converja, mientras que uno muy bajo puede hacer que el entrenamiento sea muy lento.\n",
        "- **Tamaño del batch (batch_size):** Define la cantidad de muestras que se procesan antes de actualizar los pesos del modelo. Un batch pequeño puede hacer el entrenamiento más ruidoso pero puede generalizar mejor, mientras que un batch grande aprovecha mejor la paralelización.\n",
        "- **Número de épocas (num_epochs):** Cantidad de veces que el modelo recorrerá todo el dataset de entrenamiento. Más épocas pueden mejorar el aprendizaje, pero también pueden provocar sobreajuste si es excesivo.\n",
        "\n",
        "Estos parámetros se eligen con base en la experiencia, la capacidad del hardware y las características del dataset, y pueden ajustarse posteriormente para mejorar el rendimiento.\n"
      ],
      "metadata": {
        "id": "T3g896bEvJ9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiperparámetros\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "hiX-pDc8vFnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2. Seleccionar función de pérdida y optimizador**\n",
        "\n",
        "En el Paso 2 definimos el modelo `GatosPerrosCNN`, que termina con una capa de salida con activación **sigmoide** para obtener una probabilidad entre 0 y 1.\n",
        "\n",
        "Para entrenar un modelo de clasificación binaria, es clave elegir la función de pérdida adecuada según la salida del modelo:\n",
        "\n",
        "- Si el modelo **ya aplica sigmoide** en la última capa (como `GatosPerrosCNN` del Paso 2), se debe usar **`BCELoss`**, que espera probabilidades como entrada.\n",
        "\n",
        "- Si prefieres usar una función de pérdida más estable numéricamente, puedes optar por **`BCEWithLogitsLoss`**, pero en ese caso debes **eliminar la sigmoide en el modelo** y entregar directamente los logits sin transformar. Esta función combina internamente la sigmoide y la pérdida.\n",
        "\n",
        "Como optimizador, utilizamos **Adam**, que ajusta el aprendizaje para cada parámetro y suele ser eficiente para redes convolucionales.\n"
      ],
      "metadata": {
        "id": "TMQgBBbfvWPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Modelo previamente definido: model = GatosPerrosCNN(...)\n",
        "model = GatosPerrosCNN()\n",
        "\n",
        "# Función de pérdida para clasificación binaria\n",
        "criterion = nn.BCELoss()  # o nn.BCEWithLogitsLoss() si la salida no pasa por sigmoid\n",
        "\n",
        "# Optimizador Adam\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "dVkP5IB_vZVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4. Registro de métricas de entrenamiento y validación por época**\n",
        "\n",
        "En este bloque de código implementamos el ciclo completo de entrenamiento y validación para cada época, incluyendo el cálculo y registro de métricas clave para evaluar el desempeño del modelo.\n",
        "\n",
        "### Detalles importantes:\n",
        "\n",
        "- **Fijamos una semilla para reproducibilidad** usando `set_seed()`, asegurando que los resultados sean consistentes entre ejecuciones.\n",
        "- **Configuramos el dispositivo** para aprovechar GPU si está disponible, acelerando el entrenamiento.\n",
        "- Durante el **entrenamiento** (`model.train()`):\n",
        "  - Recorremos los batches del conjunto de entrenamiento.\n",
        "  - Movemos datos y etiquetas al dispositivo.\n",
        "  - Reseteamos gradientes (`optimizer.zero_grad()`).\n",
        "  - Calculamos la salida del modelo y la pérdida.\n",
        "  - Retropropagamos el error (`loss.backward()`) y actualizamos los pesos (`optimizer.step()`).\n",
        "  - Acumulamos la pérdida total y guardamos las predicciones y etiquetas reales para calcular métricas al final de la época.\n",
        "- Calculamos las **métricas de entrenamiento** al finalizar la época:\n",
        "  - Pérdida promedio\n",
        "  - Precisión (Precision)\n",
        "  - Sensibilidad o recall (Recall)\n",
        "  - F1-score, que combina precisión y recall en una sola medida balanceada.\n",
        "- Durante la **validación** (`model.eval()`):\n",
        "  - Desactivamos el cálculo de gradientes con `torch.no_grad()` para ahorrar memoria y cómputo.\n",
        "  - Repetimos un proceso similar al entrenamiento pero sin optimización, solo evaluación.\n",
        "  - Calculamos las mismas métricas para medir la capacidad del modelo en datos no vistos durante el entrenamiento.\n",
        "- Finalmente, imprimimos un resumen formateado con el tiempo transcurrido por época y todas las métricas calculadas.\n",
        "\n",
        "### Métricas clave:\n",
        "\n",
        "- **Precisión (Precision):** proporción de predicciones positivas correctas entre todas las predicciones positivas.\n",
        "- **Recall:** proporción de verdaderos positivos detectados entre todos los positivos reales.\n",
        "- **F1-score:** media armónica de precisión y recall, útil para datasets desbalanceados.\n",
        "\n",
        "Este registro por época permite monitorear el aprendizaje del modelo y detectar posibles problemas como overfitting (cuando la validación empeora mientras que el entrenamiento mejora).\n"
      ],
      "metadata": {
        "id": "9xMzCO9rvlKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# --- Fijamos la semilla para reproducibilidad ---\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# --- Configuración del dispositivo ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# --- Loop principal de entrenamiento ---\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # ===== ENTRENAMIENTO =====\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_preds = []\n",
        "    train_labels = []\n",
        "\n",
        "    # Iteramos sobre los batches del conjunto de entrenamiento\n",
        "    for inputs, labels in train_loader:\n",
        "        # Movemos los datos al dispositivo (GPU o CPU)\n",
        "        inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "\n",
        "        optimizer.zero_grad()          # Reiniciamos los gradientes\n",
        "        outputs = model(inputs).squeeze()  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Cálculo de la pérdida\n",
        "        loss.backward()               # Backpropagation\n",
        "        optimizer.step()              # Actualizamos los pesos\n",
        "\n",
        "        train_loss += loss.item() * inputs.size(0)  # Acumulamos la pérdida\n",
        "        preds = (outputs > 0.5).int()               # Predicciones binarias (umbral 0.5)\n",
        "        train_preds.extend(preds.cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Métricas de entrenamiento por época\n",
        "    epoch_train_loss = train_loss / len(train_loader.dataset)\n",
        "    epoch_train_precision = precision_score(train_labels, train_preds)\n",
        "    epoch_train_recall = recall_score(train_labels, train_preds)\n",
        "    epoch_train_f1 = f1_score(train_labels, train_preds)\n",
        "\n",
        "    # ===== VALIDACIÓN =====\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():  # Desactivamos el cálculo de gradientes\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            preds = (outputs > 0.5).int()\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Métricas de validación por época\n",
        "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "    epoch_val_precision = precision_score(val_labels, val_preds)\n",
        "    epoch_val_recall = recall_score(val_labels, val_preds)\n",
        "    epoch_val_f1 = f1_score(val_labels, val_preds)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed = end_time - start_time\n",
        "\n",
        "    # ===== PRINT FORMATEADO POR ÉPOCA =====\n",
        "    print(f\"🧠 Epoch {epoch+1}/{num_epochs} - ⏱️ Tiempo: {elapsed:.2f}s\")\n",
        "    print(f\"   🔹 Train -> Loss: {epoch_train_loss:.2f}, Precision: {epoch_train_precision:.2f}, Recall: {epoch_train_recall:.2f}, F1: {epoch_train_f1:.2f}\")\n",
        "    print(f\"   🔸 Valid -> Loss: {epoch_val_loss:.2f}, Precision: {epoch_val_precision:.2f}, Recall: {epoch_val_recall:.2f}, F1: {epoch_val_f1:.2f}\\n\")"
      ],
      "metadata": {
        "id": "XXEq8B__vnpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------\n",
        "## 3.5 Implementación de Early Stopping basado en rendimiento de validación\n",
        "\n",
        "### ¿Qué es Early Stopping?\n",
        "\n",
        "Early stopping es una técnica que detiene el entrenamiento de un modelo cuando la métrica de evaluación en el conjunto de validación deja de mejorar después de un número determinado de épocas (llamado *patiencia*). Esto ayuda a prevenir el sobreajuste y a optimizar el uso de recursos computacionales.\n",
        "\n",
        "---\n",
        "\n",
        "### Funcionamiento en el código\n",
        "\n",
        "1. **Variable `best_f1`:**  \n",
        "   Guarda el mejor valor de F1-score observado en el conjunto de validación. Se inicializa en 0.\n",
        "\n",
        "2. **Parámetro `patience`:**  \n",
        "   Número de épocas consecutivas sin mejora permitidas antes de detener el entrenamiento (en este caso, 3).\n",
        "\n",
        "3. **Durante cada época de entrenamiento:**\n",
        "\n",
        "   - Se calcula el F1-score en validación (`epoch_val_f1`).\n",
        "   - Si `epoch_val_f1` es mejor que `best_f1`:  \n",
        "     - Se actualiza `best_f1` con el nuevo valor.  \n",
        "     - Se guarda el estado actual del modelo (`best_model.pth`).  \n",
        "     - Se reinicia el contador de épocas sin mejora (`counter = 0`).\n",
        "   - Si no hay mejora:  \n",
        "     - Se incrementa el contador (`counter += 1`).\n",
        "\n",
        "4. **Condición de parada:**  \n",
        "   Si `counter` alcanza el valor de `patience`, el entrenamiento se detiene anticipadamente (`break`).\n",
        "\n",
        "5. **Al final:**  \n",
        "   Se carga el mejor modelo guardado para usarlo en evaluaciones o inferencia posteriores.\n",
        "\n",
        "### Beneficios\n",
        "\n",
        "- Evita sobreentrenar el modelo más allá del punto óptimo.  \n",
        "- Reduce tiempo y recursos de entrenamiento.  \n",
        "- Garantiza que se utilice el modelo con mejor rendimiento en validación.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xITbMzkEVz4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# --- Fijamos la semilla para reproducibilidad ---\n",
        "# Esto asegura que los resultados sean iguales cada vez que se ejecuta el script\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# --- Configuración del dispositivo (GPU si está disponible) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# --- Parámetros de Early Stopping ---\n",
        "best_f1 = 0.0              # Mejor F1 observado hasta el momento\n",
        "patience = 3               # Nº de épocas sin mejora antes de frenar\n",
        "counter = 0                # Cuántas épocas seguidas no mejoró\n",
        "best_model_path = \"best_model.pth\"  # Donde se guarda el mejor modelo\n",
        "best_epoch = 0             # Época donde se logró el mejor F1\n",
        "\n",
        "# --- Ciclo principal de entrenamiento ---\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # --- Entrenamiento ---\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_preds = []\n",
        "    train_labels = []\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Pasamos datos a dispositivo\n",
        "        inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "\n",
        "        # Paso de forward + backward\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Acumulamos métricas\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        preds = (outputs > 0.5).int()\n",
        "        train_preds.extend(preds.cpu().numpy())\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Cálculo de métricas de entrenamiento\n",
        "    epoch_train_loss = train_loss / len(train_loader.dataset)\n",
        "    epoch_train_precision = precision_score(train_labels, train_preds)\n",
        "    epoch_train_recall = recall_score(train_labels, train_preds)\n",
        "    epoch_train_f1 = f1_score(train_labels, train_preds)\n",
        "\n",
        "    # --- Validación ---\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
        "            outputs = model(inputs).squeeze()\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            preds = (outputs > 0.5).int()\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Cálculo de métricas de validación\n",
        "    epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "    epoch_val_precision = precision_score(val_labels, val_preds)\n",
        "    epoch_val_recall = recall_score(val_labels, val_preds)\n",
        "    epoch_val_f1 = f1_score(val_labels, val_preds)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed = end_time - start_time\n",
        "\n",
        "    # --- Impresión de métricas por época ---\n",
        "    print(f\"🧠 Epoch {epoch+1}/{num_epochs} - ⏱️ Tiempo: {elapsed:.2f}s\")\n",
        "    print(f\"   🔹 Train -> Loss: {epoch_train_loss:.2f}, Precision: {epoch_train_precision:.2f}, Recall: {epoch_train_recall:.2f}, F1: {epoch_train_f1:.2f}\")\n",
        "    print(f\"   🔸 Valid -> Loss: {epoch_val_loss:.2f}, Precision: {epoch_val_precision:.2f}, Recall: {epoch_val_recall:.2f}, F1: {epoch_val_f1:.2f}\")\n",
        "\n",
        "    # --- Lógica de Early Stopping ---\n",
        "    if epoch_val_f1 > best_f1:\n",
        "        best_f1 = epoch_val_f1\n",
        "        best_epoch = epoch + 1\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"✅ Mejor F1 en validación: {best_f1:.2f} → Modelo guardado.\\n\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"⏸️ Sin mejora en F1 ({counter}/{patience})\\n\")\n",
        "        if counter >= patience:\n",
        "            print(\"⛔ Early stopping activado.\")\n",
        "            break\n",
        "\n",
        "# --- Restaurar el mejor modelo antes de evaluar o usar ---\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "print(f\"✅ Modelo restaurado desde la epoch {best_epoch} con mejor F1: {best_f1:.2f}\")"
      ],
      "metadata": {
        "id": "llblUD2SWAgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusión\n",
        "El modelo muestra una mejora consistente en las métricas de validación durante las primeras épocas, especialmente en F1-score, que alcanza un máximo de 0.89. El entrenamiento logra mantener una buena precisión y recall, con pérdidas bajas tanto en entrenamiento como en validación.\n",
        "\n",
        "Se observa que el early stopping funcionó correctamente, ya que el entrenamiento se detuvo tras detectar varias épocas sin mejora significativa en el F1 de validación, evitando así el sobreajuste. Además, el mejor modelo fue guardado y restaurado exitosamente para su uso posterior.\n",
        "\n",
        "En resumen, el modelo entrenado es robusto y el procedimiento de early stopping ayuda a optimizar el rendimiento y el tiempo de entrenamiento."
      ],
      "metadata": {
        "id": "4YLhwC9XDfGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias\n",
        "\n",
        "- **Material de clases**  \n",
        "  Base para la estructura del proyecto, manejo de datasets y entrenamiento con PyTorch.\n",
        "\n",
        "- **Prompts utilizados con GPT para mejorar el código y obtener sugerencias:**\n",
        "\n",
        "  - *Descarga y descompresión segura y eficiente:*  \n",
        "    \"Necesito un script en Python que descargue un archivo ZIP desde una URL y lo descomprima, pero que primero verifique si el archivo ya está descargado y la carpeta ya está descomprimida para no repetir el proceso y no ralentizar la ejecución. Que sea simple y con mensajes claros de estado.\"\n",
        "\n",
        "  - *Dataset personalizado en PyTorch para imágenes de gatos y perros con transformaciones básicas:*  \n",
        "    \"¿Podés mostrarme cómo implementar un Dataset personalizado con PyTorch para un conjunto de imágenes de gatos y perros, y cómo usarlo con un DataLoader que incluya transformaciones como resize y conversión a tensor?\"\n",
        "\n",
        "  - *Transformaciones recomendadas para dataset de imágenes en PyTorch:*  \n",
        "    \"Estoy trabajando con el dataset de gatos y perros. ¿Qué transformaciones debería aplicar con PyTorch (resize, normalización y data augmentation) para usar en un DataLoader personalizado?\"\n",
        "\n",
        "  - *Control de aleatoriedad para reproducibilidad:*  \n",
        "    \"¿Cómo aplicar una semilla fija para random_split en PyTorch?\"\n",
        "\n",
        "  - *División estratificada del dataset para mantener proporción de clases:*  \n",
        "    \"¿Cuál es la mejor manera de mantener la proporción de clases al dividir un dataset para entrenamiento y validación?\"\n",
        "\n",
        "  - *Detección y filtrado de imágenes corruptas en datasets de imágenes:*  \n",
        "    \"¿Cómo detectar y filtrar imágenes corruptas en un dataset de imágenes en PyTorch?\"  \n",
        "    \"¿Cómo validar que las imágenes de un dataset estén en formato RGB y no corruptas antes de usarlas?\"\n",
        "\n",
        "  - *Buenas prácticas para limpieza de datasets de imágenes:*  \n",
        "    \"¿Qué criterios puedo usar para limpiar un dataset de imágenes antes del entrenamiento?\"\n",
        "\n",
        "  - *Configuración de hiperparámetros para entrenamiento en PyTorch:*  \n",
        "    \"¿Cómo configuro correctamente el learning rate, batch size y número de épocas para entrenar un modelo en PyTorch?\"\n",
        "\n",
        "  - *Función de pérdida y optimizador para clasificación binaria:*  \n",
        "    \"¿Cuál es la función de pérdida más adecuada para un problema de clasificación binaria?\"\n",
        "\n",
        "  - *Implementación de early stopping en PyTorch para control de sobreajuste:*  \n",
        "    \"¿Cómo implemento early stopping en PyTorch para detener el entrenamiento cuando la métrica de validación deja de mejorar?\"\n"
      ],
      "metadata": {
        "id": "-mED4bzyNSyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____________\n",
        "______________\n",
        "_____________"
      ],
      "metadata": {
        "id": "wiYwcI-VY6KH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}